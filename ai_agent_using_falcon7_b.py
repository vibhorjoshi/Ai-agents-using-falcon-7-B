# -*- coding: utf-8 -*-
"""AI agent using falcon7-B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BmVnDZpBEHO1kP68lTy26XyHHXYb1BV4
"""

# Install necessary libraries
!pip install langchain transformers torch beautifulsoup4 requests accelerate
!pip install langchain_community
!pip install -U bitsandbytes
# Import required modules
import requests
from bs4 import BeautifulSoup
from langchain.llms import HuggingFacePipeline
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
from accelerate import init_empty_weights
from transformers import BitsAndBytesConfig


# Load Falcon-7B Model from Hugging Face
model_name = "tiiuae/falcon-7b-instruct"
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_name)
# Load Falcon-7B with reduced precision
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="cpu", offload_folder="offload")

# Create the text generation pipeline
summarizer = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Wrap the model for LangChain
llm = HuggingFacePipeline(pipeline=summarizer)

# Function to scrape website content
def scrape_website(url):
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        text = ' '.join([p.text for p in soup.find_all('p')])  # Extract text from <p> tags
        return text
    else:
        return f"‚ùå Failed to fetch website. Status code: {response.status_code}"

# Function to analyze and summarize scraped content
def analyze_content(url):
    scraped_text = scrape_website(url)

    if "Failed" in scraped_text:
        return scraped_text  # Return error message if scraping failed

    # Limit text length to 1024 tokens (model constraint)
    input_text = scraped_text[:1024]

    # Generate summary using Falcon-7B
    summary = llm(f"Summarize the following text:\n{input_text}")
    return summary

# User Input for URL
url = input("üîó Enter the URL to scrape: ")
summary = analyze_content(url)

# Print Summary
print("\nüìÑ Webpage Summary:\n", summary)